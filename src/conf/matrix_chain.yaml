inherit: 
    - wandb.yaml

model:
    family: transformer_group  # Use standard GPT2 transformer
    n_dims: 4
    n_positions: 100  # L * (n^2 + 2n) = 3 * (16 + 8) = 72, use a bit less for buffer
    n_embd: 256
    n_layer: 12
    n_head: 8

model:
    n_dims: 2  # n+p+q (4+4+4)
    n_positions: 200  # L * (n+p+q) = 3 * 12

training:
    task: matrix_chain
    data: matrix_chain
    task_kwargs: {"L": 50, "n": 2, "m": 2, "p": 2, "q": 2}
    batch_size: 64
    learning_rate: 0.0001
    train_steps: 50000
    curriculum:
        dims:
            start: 2
            end: 2
            inc: 0
            interval: 2000
        points:
            start: 1000
            end: 1000
            inc: 0
            interval: 2000

out_dir: ../models/matrix_chain

wandb:
    name: "matrix_chain_standard"

